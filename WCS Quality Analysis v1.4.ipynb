{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# Watson Conversation Quality Analysis\n\nThis notebook describes an approach to evaluate a chatbot's classification quality when using Watson Conversation Service (WCS). \n\nIt focuses only on ML evaluation regarding intents (an advanced analysis would consider entities also) and it presents and discusses some data science concepts for better understanding.\n\n* **Author: Renato dos Santos Leal**\n* **Linkedin: https://www.linkedin.com/in/renatodossantosleal/**\n* **Medium: http://medium.com/@renatoleal**", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Summary\n\n* [Prerequisites](#prereqs)\n* [Step 1 - Required libraries to be imported](#setup)\n* [Step 2 - WCS Credentials](#credentials)\n* [Step 3 - **Exploratory analysis**](#exploratory)\n    * [Intent Distribution](#intentdistribution)\n    * [Imbalanced Samples and Accuracy Paradox](#adtpa)\n    * [Minimum of Examples per Intent](#intentmin)\n    * [Repeated Examples](#repeatedexamples)\n    * [Log history analysis](#loganalysis)\n    * [Overall Mean Confidence](#meanconfidence)\n    * [Distribution of log intents and mean confidence by intent](#logdistribution)\n    * [Low Confidence Examples](#lowconfidence)\n    * [Most Frequent Questions](#manyquestions)\n<br/><br/>\n* [Step 4 - **Advanced Analysis**](#advanced)\n    * [Amostragem](#amostragem)\n  ", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## <a id=\"prereqs\"></a>Prerequisites\n\nFor better undestanding and use of this notebook you must know (with example of courses/articles):\n* **Python** (https://cognitiveclass.ai/courses/introduction-to-python/)\n* **Jupyter Notebooks** (Module 2: https://cognitiveclass.ai/courses/data-science-hands-open-source-tools-2/)\n* **ML evaluation metrics (in portuguese)** (Articles 1, 2 e 3: http://bit.ly/2keKqUt)\n* **Watson Conversation (in portuguese)** (https://medium.com/as-m%C3%A1quinas-que-pensam/criando-chat-bots-no-facebook-com-o-ibm-watson-351df84e653d)\n\nPs: This notebook runs on Python 3.5 and Spark 2.0 or 2.1.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## <a id=\"setup\"></a> Step 1 - Required libraries to be imported\nInstalls, imports and updates required libraries.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Update/installs scikit-learn and termcolor \n# !pip install -U scikit-learn\n# !pip install termcolor\n\n# Supporting Libs\nimport re\nimport os\nimport sys\nimport json\nimport time\nimport nltk\nimport sklearn\nimport itertools\nimport matplotlib\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n# Watson APIs Libs\nfrom watson_developer_cloud import ConversationV1\n\n# Metrics & ML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\n# Visualization configs\nfrom termcolor import colored, cprint\nfrom IPython.display import display, HTML\n%matplotlib inline\nmatplotlib.style.use('ggplot')\npd.options.display.max_colwidth = 150", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## <a id=\"credentials\"></a>Step 2 - WCS Credentials & Auxiliary Functions\nTo obtain your WCS credentials you must access the Deploy screen at the left side of your workspace and then click on \"credentials\". ** (Not a good practice fixing your credentials on code!) **", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "CTE_WORKSPACE = \"{workspace}\"\nCTE_USERNAME = \"{username}\"\nCTE_PASSWORD = \"{password}\"\n\nconversation = ConversationV1(\n    url=\"https://gateway.watsonplatform.net/conversation/api\",\n    username=CTE_USERNAME,\n    password=CTE_PASSWORD,\n    version='2017-05-26'\n)\n\noriginal_workspace_id = CTE_WORKSPACE", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# The code was removed by DSX for sharing.", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Auxiliary Functions\nJust some functions to make it easier to read: the first one gets a workspace from WCS and checks its status, the second one just prints to console in red.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Check if workspace is ready to recieve calls, if it is not then wait for 30 seconds and try again. This function blocks the rest of the code!\ndef check_wksp_status(check_workspace_id):\n    wksp_notready = True\n    \n    while(wksp_notready == True):\n        print('Testing the workspace...')\n        workspace = conversation.get_workspace(workspace_id=check_workspace_id)\n\n        print('Workspace Status: {0}'.format(workspace['status']))\n        if workspace['status'] == 'Available':\n            wksp_notready = False\n            print('Ready to use!')\n        else:\n            print('In training...wait 30s then try again.')\n            time.sleep(30)\n\n# Prints logs in red and in bold\ndef printred(str_temp,isbold):\n    if isbold:\n        print(colored(str_temp, 'red', attrs=['bold']))\n    else:\n        print(colored(str_temp, 'red'))", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## <a id=\"exploratory\"></a>Step 3 - Exploratory analysis\n\n**PS 1:** when working with data science and machine learning it is usually a good practice to have a well-defined methodology in place to guide you through the process of creating your solution. A well-known methodology in market for years is CRISP-DM and it is the one we chose this notebook. \n\n**PS 2:** if you want to know more about some methodology used in data science project I would recommend this one: https://cognitiveclass.ai/courses/data-science-methodology-2/.\n\n### Intro\nAn important step of the development of a project using Watson is understanding the data you have and the training you're doing, having a clear understanding of the choices you make during each step will define the success of your project. The techniques presented here will help you with the evaluation stage of CRISP-DM but could be used for data understanding and model training.\n\nThe data to be analyzed is divided into two categories:\n* **Training data**: the intents, examples and entities of our chatbot\n* **Historical data**: user inputs sent to our bot.\n\nTo understand the data used in training and interpret it accordingly we'll first do an exploratory analysis based on example quantity and its distribution on each intent, after that we'll focus on an historical analysis with similar techniques.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Exporting the workspace\nExports all of your chatbot content so we may be able to use it without need to do additional calls to the service.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# It is necessary to use the export flag as True, otherwise only metadata will be returned\noriginal_workspace = conversation.get_workspace(workspace_id=original_workspace_id, export=True)\n\n# Calls the auxiliary function to check if the workspace is ready\ncheck_wksp_status(original_workspace_id)", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <a id=\"intentdistribution\"></a>Intent Distribution\n\nThis is a visual analysis that presents to us how many examples we have in each intent, it helps to understand what kind of questions we're answering and if there is any kind of discrepancy on the size of the training data.\n\n**Questions to have in mind when analyzing the chart:**\n* Is there an intent that appears a lot more than others? \n* Why does this happen? \n* Is there an intentn that is too generic? \n* Are our \"retraining efforts\" focusing on this and forgetting others?\n\n**PS:** by experience when an intent has 3 or 4 times more examples than the others then it usually is too generic. Intent definition approaches will vary a lot depending on the project, using entities will give you more capilarity but even with that you may find yourself with an intent that has 200+ examples.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Get intent list from previous json export\nlist_original_intents = original_workspace['intents']\nlist_original_examples = []\nlist_original_intent_names = []\n\n# Variable declaration\nintent_distribution = pd.DataFrame(columns=['classes', 'size'])\navg_size = 0;\n\n# Mounts distribution visualization\nfor idx, intent in enumerate(list_original_intents):\n    for example in intent['examples']:\n        list_original_examples.append(example['text'])\n        list_original_intent_names.append(intent['intent'])\n    intent_distribution.loc[idx] = pd.Series({'classes':intent['intent'], 'size': len(intent['examples'])})\n    avg_size = avg_size + len(intent['examples'])\n\n# Prints the chart on screen\nintent_distribution.plot(kind='bar',x='classes', y='size',figsize=(30,7))\n\n# Mounts the data frame\nintent_distribution = pd.DataFrame({\n    'Example': list_original_examples,\n    'Intent': list_original_intent_names\n}, columns=['Example','Intent'])", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Average intent size\nfinal_avg_size = avg_size/len(list_original_intents)\n\nprint(\"Average intent size: \" + str(final_avg_size))", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Why concentrating examples into one intent only is not a good idea:\n#### <a id=\"adtpa\"></a>Imbalanced Samples and Accuracy Paradox\n\nA common problem we encounter when working with data is related to the fact that the number of occurrences of a given event is, usually, not balanced. In our case people tend to train chatbots without considering the number of examples per intent and then find itself with a bot that has lots of examples for one or two intents and just a little for the rest of it.\n\nIn data science we call it an **imbalanced training sample** and its impact on our model quality happens because some distortions are created regarding its udnerstanding. Consider the following:\n\n1. We have a training set with 90 positive examples and 10 negative examples.\n2. Your model gets trained with it and assumes that every input should be classified as positive.\n3. **Is this a bad model?**\n4. **Not necessarily**, your classifier has a 90% accuracy which is quite good!\n5. **Is it helpful?** Probably not.\n\nSo this is called the **accuracy paradox** (https://en.wikipedia.org/wiki/Accuracy_paradox), having a model that presents good metrics witha training sample but it is not really useful. (You can learn more in here: https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/)\n\nYou may also learn something about imbalanced samples and how to treat it with these links:\n* https://svds.com/learning-imbalanced-classes/ \n* https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n\n**So, from what we've seen that it is good to keep your training sample with a similar amount of examples!** Let's see how our classifier is doing regarding this.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Discrepancy coeficient\ncte_coef_disc = 0.5\n\nprint(colored(\"\\nIntents with a discrepancy of examples (intent # size):\\n\", attrs=['bold']))\n\nif final_avg_size < 5:\n    print(colored(\">>> The presented sample does not meet the minimum required for training (5 examples) and therefore the deviations will not be calculated.\", 'red', attrs=['bold']))\nelse:\n    # Valida\u00e7\u00e3o de quais classes est\u00e3o \"ofendendo\" a distribui\u00e7\u00e3o\n    for intent in list_original_intents:\n        diff = float(len(intent['examples'])) - final_avg_size\n        if(abs(diff) > (final_avg_size * cte_coef_disc)):\n            if diff > 0:\n                printred(\"[+] >>> \" + intent['intent'] + ' # ' + str(len(intent['examples'])) + ' / Has ' + str(round(diff-(final_avg_size * cte_coef_disc),2)) + ' more examples than expected.',True)\n            else:\n                printred(\"[-] >>> \" + intent['intent'] + ' # ' + str(len(intent['examples'])) + ' / Has ' + str(round(abs(diff)-(final_avg_size * cte_coef_disc),2)) + ' less examples than expected.',True)\n                    ", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Done! Now we have a first glimpse on which classes to tweak in order to make our classifier more accurate but how can we improve this analysis?", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <a id=\"intentmin\"></a>Minimum examples per intent\nAnother analysis we may do to guarantee our chatbot best performance is regarding minimum requirements of the API:\n\nWatson Conversation documentation mentions the **need to have at least five examples per intent, but the suggestion is reaching up to ten examples**. Let's check if our chatbot meets those requirements:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "print(colored(\"\\nIntents without minimum amount of examples (5):\\n\", attrs=['bold']))\n\n# Checks if the required minimum has been met\nfor intent in list_original_intents:\n    if len(intent['examples']) < 5:\n        printred(\">>> \" + intent['intent'] + ' # ' + str(len(intent['examples'])),True)\n        \nprint(colored(\"\\n\\nIntents without minimum SUGGESTED amount of examples (10):\\n\", attrs=['bold']))\n\n# Checks if the suggested minimum has been met\nfor intent in list_original_intents:\n    if len(intent['examples']) < 10:\n        printred(\">>> \" + intent['intent'] + ' # ' + str(len(intent['examples'])),True)", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "** The intents that appear above should be the ones that you must focus first because they dont even meet minimum requirements. You should question the existence of those, are they really necessary? If not, then don't put it in your model, not even if plan to use it in the future.**", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <a id=\"repeatedexamples\"></a>Repeated examples\nOur last analysis on intent examples is related to repeated occurrences of it. Although it may not be prohibited, having repeated examples in more than one intent is not a good practice.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "print(colored(\"\\nSelects repeated examples in our training set:\\n\", attrs=['bold']))\n\n# Mounts example frequency\nfdist = nltk.FreqDist(intent_distribution['Example'])        \n\n# Select those with more than one occurrence\nrepeated = [x for idx,x in intent_distribution.sort_values(\"Example\").iterrows() if x['Example'] in [k for k,v in fdist.items() if v > 1]]\nfor y in repeated:\n    print(y['Example'] + ' # ' + y['Intent'])\n    \nif len(repeated) <= 2:\n    print(colored(\"No repeated examples in our set. Congrats!\", 'green'))", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## <a id=\"loganalysis\"></a>Log history analysis\nKeeping up with our **data understanding** step of the process we will now analyze our chatbot behaviour with real user inputs, the questions sent to our bot in the past.\n\nLet's start by checking intent distribution in our logs just like we've done with our examples.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Auxiliary function that calls WCS for log output\ndef mount_logs_dump(wid):\n    response = conversation.list_logs(workspace_id = wid,page_limit=500)\n\n    list_mount_examples = []\n    list_mount_intents = []\n    list_mount_intents2 = []\n    list_mount_confidence = []\n    list_mount_confidence2 = []\n    \n    cursor_regex = r\".*?cursor=(.*?)&\"\n\n    while response and 'logs' in response:\n        for log in response['logs']:\n            if 'response' in log:\n                lresponse = log['response']\n                \n                if 'input' in lresponse and 'text' in lresponse['input']:\n                    if 'intents' in lresponse and lresponse['intents']:\n                        list_mount_examples.append(lresponse['input']['text'].strip())\n                        list_mount_intents.append(lresponse['intents'][0]['intent'])\n                        list_mount_confidence.append(lresponse['intents'][0]['confidence'])\n\n                        if 'alternate_intents' in log['request'] and log['request']['alternate_intents'] == \"true\":\n                            list_mount_intents2.append(lresponse['intents'][1]['intent'])\n                            list_mount_confidence2.append(lresponse['intents'][1]['confidence'])\n                        else:\n                            list_mount_intents2.append('N/A')\n                            list_mount_confidence2.append('0')\n                    else:\n                        list_mount_examples.append(lresponse['input']['text'].strip())\n                        list_mount_intents.append('irrelevant')\n                        list_mount_confidence.append('0')\n                        list_mount_intents2.append('N/A')\n                        list_mount_confidence2.append('0')\n    \n    \n        if 'pagination' not in response or 'next_url' not in response['pagination']:\n            break\n    \n        cursor_res = re.search(cursor_regex, response['pagination']['next_url'], re.IGNORECASE)\n        cursor = None\n    \n        if cursor_res:\n            cursor = cursor_res.group(1)\n        if not cursor:\n            break\n     \n        response = conversation.list_logs(workspace_id=wid, page_limit=1000, cursor=cursor)\n    \n    df_temp = pd.DataFrame({\n        'Example': list_mount_examples,\n        'Intent_1': list_mount_intents,\n        'Confidence_1': list_mount_confidence,\n        'Intent_2': list_mount_intents2,\n        'Confidence_2': list_mount_confidence2,\n    }, columns=['Example','Intent_1','Confidence_1','Intent_2','Confidence_2'])\n    \n    return df_temp", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "scrolled": false
            }, 
            "source": "# Gather logs from all conversations made to the workspace\nlist_logs = mount_logs_dump(original_workspace_id)\n\n# Check if there are logs at all\nflag_logs = len(list_logs) > 0", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Mounts our data frame for later visualization\nif flag_logs:\n    dist_logs = pd.DataFrame(columns=['intent', 'sizes','confidencesum'])\n\n    # No need to analyze those\n    intent_blacklist = ['greetings','fim']\n\n    for idx,log in list_logs.iterrows():\n        if log['Intent_1'] not in intent_blacklist:\n            dist_logs.loc[idx] = pd.Series({'intent': log['Intent_1'], 'sizes': 1,'confidencesum': float(log['Confidence_1'])})", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <a id=\"meanconfidence\"></a>Overall Mean Confidence\n\nWe could look only at the overall mean confidence of our model regarding user inputs it recieved (and that's an error a lot of people do). **But, as any metric or technique, we should think about what it actuallt represents.**\n\nSo let's imagine two different situations:\n\n### Situation 1 - Bad testing distribution\nI've got a chatbot with only one well-trained intent and all of the other intents have a really poor training (remmember the imbalanced sample example?). **We should get a bad confidence number, right?** <font color='red'>**Not really.**</font>\n\n* If checked only with our log history and every input was made against the well trained intent then we would have a misleading confidence index with a high number on it. That does not represents reallity of our training.\n* **Does this mean our bot is not good enough?** No. It actually responds what it needs most of the time but gets lost with everything else. It does the just, it just isn't that smart.\n\n**When does this usually happens?** We see this type of behaviour when the same team creates and tests the bot. They know what the bot is supposed to answer and are biased to restrict itself within limitations of the training. <font color='red'>**That's why you should test you bot with people that haven't seen it before.**</font>\n\n### Situation 2 - High confidence false positives\n**This is somehow more critical because it can lead to image damages to the company if it happens:**\n\nwe're talking about false positives, i.e., when your bot returns high confidences for wrong intents/answers. Imagine the same bot as before but with the following result:\n* Your users are asking lots of different questions for the remaining intents (not the one with lots of examples) and it always returns that same intent with high confidence (let's just say it assumes that every input belongs to that class and returns with 85% confidence). **Pretty bad huh?**\n* Now imagine the intent returned has the \"It is always nice talking to you\" answer and the user wanted to complain or cancel. Or even worse, the answer comes as something aggressive for someone.\n\n\n** This is a great example that shows us that using returned confidence without context may give you the opposite effect of the measuring. **", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Calculates overall mean confidence\nif flag_logs:\n    print(\"Mean confidence (with irrelevants) > \" + str(round(dist_logs['confidencesum'].mean()*100,2)) + \" % \")\n    print(\"Mean confidence (without irrelevants) > \" + str(round(dist_logs[dist_logs['intent'] != 'irrelevant']['confidencesum'].mean()*100,2)) + \" % \")", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "**So we have seen that the overall average confidence may not be a good demonstration of the general accuracy level of our bot.**", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <a id=\"logdistribution\"></a>Log Distribution and Mean Confidence by Intent\n\nNow that we discussed what confidence really means and problems it could bring, we can make a smarter analysis of it using two charts:\n* Distribution of most asked questions.\n* Mean confidence by intent.\n\nWith those in hands we may find most asked questions that had lower confidence than expected and prioritize it.\n\n**Good questions would be:**\n* Does our main intents reach a minimum level defined by us?\n* Which intent should I focus on first?\n* For those with less than minimum: do we have enough new inputs to improve retrain accordingly?\n\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Groups intents, sum occurences and calculate mean confidence for each.\nif flag_logs:\n    \n    # Groups intents\n    grouped1 = dist_logs.groupby('intent').mean()\n    grouped2 = dist_logs.groupby('intent').count()\n    \n    cte_logs = 5\n\n    print(colored('Most asked intents with more than ' + str(cte_logs) + ' examples\\n', attrs=['bold']))\n    grouped4 = grouped2.where(lambda x : x['sizes'] > cte_logs).dropna().sort_values(by='sizes')\n    \n    grouped4.index.name='intent'\n    grouped4['intent']=grouped4.index\n    \n    # Shows the distribution log on screen\n    grouped4.plot(kind='bar',x='intent', y='confidencesum',figsize=(30,7),title='Most asked intents with more than ' + str(cte_logs) + ' examples')", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "if flag_logs:\n    # Computes the intersection between the series\n    df_intersection = pd.merge(grouped4, grouped1, left_index=True, right_index=True)\n    df_intersection.drop('confidencesum_x', axis=1, inplace=True)\n    \n    print(colored('Average confidence by intent (with more than ' + str(cte_logs) + ' examples):\\n', attrs=['bold']))\n    display(df_intersection.sort_values(by='confidencesum_y'))", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Prints chart with log distribution vs. overall confidence\n\nif flag_logs:\n    df_intersection.sort_values(by='confidencesum_y').plot(kind='bar',x='intent', y='confidencesum_y',figsize=(30,7),title='Average confidence by intent (with more than ' + str(cte_logs) + ' examples)')", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Prints combined charts\nif flag_logs:\n    # Removes unnecessary columns\n    grouped4.drop('confidencesum', axis=1, inplace=True)\n    df_intersection.drop('sizes_y', axis=1, inplace=True)\n\n    ax = grouped4.plot(kind='line',x='intent', y='sizes',figsize=(30,7),color='b',linewidth=3,secondary_y=True,legend=True)\n    df_intersection.plot(ax=ax, kind='bar',x='intent', y='confidencesum_y',figsize=(30,7),title='Average confidence by intent x size',legend=True)\n    \n    plt.gcf().autofmt_xdate()\n    plt.show()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### (Understanding the chart above)\nA nice approach here would be defining a minimum threshold for expected confidence and then start analyzing intent by intent from right to left.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <a id=\"lowconfidence\"></a>Examples with Low Confidence\n\n<font color='red'>[TODO: Translate]</font>\n\n\nDependendo da quantidade de intera\u00e7\u00f5es que o seu bot j\u00e1 teve iniciar um retreino pode parecer uma tarefa assustadora. Uma boa pr\u00e1tica \u00e9 come\u00e7ar o retreino do seu Conversation usando aqueles exemplos que tiveram um n\u00edvel de confian\u00e7a muito baixo, essas intera\u00e7\u00f5es servir\u00e3o para que voc\u00ea verifique a necessidade de criar novas inten\u00e7\u00f5es e/ou adicionar mais exemplos as inten\u00e7\u00f5es j\u00e1 existentes, lembrando que estes exemplos trar\u00e3o mais impacto ao treinamento do que aqueles com alto n\u00edvel de confian\u00e7a.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "MIN_CONFIDENCE = 0.5\n\nif flag_logs:\n    counter = 0\n    arr_inputs = []\n    arr_intents = []\n    arr_confidence = []\n    arr_repeated = []\n\n    for idx,log in list_logs.iterrows():\n        current_intent = log['Intent_1']\n\n        if current_intent not in intent_blacklist and float(log['Confidence_1']) < MIN_CONFIDENCE:\n            arr_repeated.append(log['Example'])\n\n            if float(log['Confidence_1']) < MIN_CONFIDENCE and log['Example'] not in arr_inputs:\n                counter = counter + 1\n                arr_inputs.append(log['Example'])\n                arr_intents.append(log['Intent_1'])\n                arr_confidence.append(log['Confidence_1'])\n    \n    low_confidence = pd.DataFrame({\n        'Example': arr_inputs,\n        'Intent': arr_intents,\n        'Confidence': arr_confidence,\n    }, columns=['Example','Intent','Confidence'])\n\n    display(low_confidence[0:20])", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <a id=\"manyquestions\"></a>Most Frequent Questions\n\nAnother good starting point to retrain your chatbot could be those questions asked more than once without a good confidence level returned.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "if flag_logs:\n    print(colored(\"\\nSelects those examples with higher occurence (more than 2 repetitions) and lower confidence:\\n\", attrs=['bold']))\n\n    fdist = nltk.FreqDist(arr_repeated)\n    flag = False\n\n    for k,v in sorted(fdist.items(), key=lambda t:t[-1], reverse=True):\n        if v > 2:\n            flag = True\n            print(\"[\" + str(v) + \"] > \",k)\n    if flag is False:\n        print(\"No repeated logs were found.\")", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## <a id=\"advanced\"></a>Step 4 - Advanced Analysis\n\n<font color='red'>[TODO: Translate]</font>\n\nPara que possamos a analisar a qualidade do nosso chatbot com um pouco mais de profundidade precisamos antes conhecer alguns conceitos:\n\nPrimeiro temos que ter em mente que a tarefa de identificar em qual classe (inten\u00e7\u00e3o) um input (exemplo) pertence \u00e9 uma tarefa de classifica\u00e7\u00e3o estat\u00edstica (https://en.wikipedia.org/wiki/Statistical_classification) e portanto devemos utilizar m\u00e9todos espec\u00edficos para fazer a avalia\u00e7\u00e3o deste tipo de tarefa.\n\nAinda sobre conceitos, considere o problema de verificar se um input pertence a inten\u00e7\u00e3o #resetarSenha:\n* **True positive (TP)** ou Verdadeiros Positivos (VP): casos em que retornamos a classe correta, o modelo preveu que a classe era #resetarSenha.\n* **False positives (FP)** ou Falsos positivos (FP): casos em que retornamos falando que era a classe #resetarSenha quando na verdade era outra.\n* **True Negative (TN)** ou Falsos Verdadeiros (FV): casos que retornamos que era outra classe e realmente era.\n* **False Negative (FN)** ou Falsos Negativos (FN): retornamos que era outra classe quando na verdade era #resetarSenha.\n* **Matriz de Confus\u00e3o**: traz uma an\u00e1lise visual dos conceitos apresentados acima. \n\n** Todos esses conceitos s\u00e3o melhores explorados nos seguintes links: **\n* https://medium.com/as-m%C3%A1quinas-que-pensam/m%C3%A9tricas-comuns-em-machine-learning-como-analisar-a-qualidade-de-chat-bots-inteligentes-introdu-57ff30424192\n* https://medium.com/as-m%C3%A1quinas-que-pensam/m%C3%A9tricas-comuns-em-machine-learning-como-analisar-a-qualidade-de-chatbots-inteligentes-conceitos-a5b586053973\n* https://medium.com/as-m%C3%A1quinas-que-pensam/m%C3%A9tricas-comuns-em-machine-learning-como-analisar-a-qualidade-de-chat-bots-inteligentes-m%C3%A9tricas-1ba580d7cc96\n\n** Obs: por quest\u00f5es de custos vamos fazer uma an\u00e1lise mais simples baseada em amostragem, no fim deste notebook abordaremos outros m\u00e9todos que podem ser utilizados para medir o seu chatbot com ainda mais precis\u00e3o. **", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Mounts dataframe to be worked with\nle_examples = []\nle_intents = []\n\nfor intent in list_original_intents:\n    for ex in intent['examples']:\n        le_examples.append(ex['text'])\n        le_intents.append(intent['intent'])\n        \nlist_ml_examples = pd.DataFrame({\n    'Example': le_examples,\n    'Intent': le_intents,\n}, columns=['Example','Intent'])", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### <a id=\"amostragem\"></a>Amostragem\n\n<font color='red'>[TODO: Translate]</font>\n\nComo mencionado acima vamos testar nosso bot baseado em amostragem e para isso precisamos separar os datasets entre treino e teste respeitando uma propor\u00e7\u00e3o pr\u00e9-determinada de 80/20, sendo que o conjunto de treino ser\u00e1 aquele que efetivamente utilizaremos para treinar um novo modelo e o conjunto de teste n\u00f3s utilizaremos para submeter ao novo modelo criado e verificar se o resultado \u00e9 o mesmo que o esperado.\n\nObs: esse m\u00e9todo n\u00e3o funciona t\u00e3o bem quando temos uma amostra muito pequena pois o modelo treinado ter\u00e1 poucos insumos com o que aprender.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# As we need to submit some examples to test the model this task may generate costs if you have already surpassed the ten thousand free monthly requests.\nlength_test_set = len(list_ml_examples)*0.2*3\n\nprint(colored('ATTENTION! By running this test you might be charged up to ' + str(round(length_test_set*0.00484,2)) + ' reais. Those costs are associated with ' \n              + str(round(length_test_set)) + ' calls to be made to your workspace! Type OK to accept...','red', attrs=['bold']))\n\nacceptance = input()\n\nif acceptance != 'OK':\n    print('I\\'ll continue anyway hehe...')", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "scrolled": true
            }, 
            "source": "# Auxiliary function that groups intents and mounts an array that WCS can understand. It also validates the number of examples in each intent.\ndef group_and_mount_intents(train_set):\n    \n    intents = {}\n    \n    # Groups intents\n    for idx, example in train_set.iterrows():\n        current_intent = example['Intent']\n        \n        if current_intent not in intents: \n            intents[current_intent] = []\n\n        intents[current_intent].append(example['Example'])\n        \n\n    workspace_intens = []\n    \n    # Transform intent format to the one accepted by WCS\n    for intent, examples in intents.items():\n        entry = {'intent': intent, 'examples': []}\n        if len(examples) < 10:\n            print(\"[WARNING] Intent #\" + intent + \" has fewer examples [\" + str(len(examples)) + \"] than expected and therefore is not a good example for this test.\")\n\n        for example in examples:\n            entry['examples'].append({ 'text': example })\n\n        workspace_intens.append(entry)\n\n    print('\\nIntents mounted.')\n\n    return workspace_intens", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Auxiliary function that creates a new workspace in order to run tests without damaging the current chatbot\ndef create_test_workspace(train_set):\n    intents_json = group_and_mount_intents(train_set)\n    \n    response = conversation.create_workspace(name = 'Automated_DSX_Test', entities = original_workspace['entities'], intents = intents_json, language = 'pt-br')\n    print('Workspace created, waiting to be ready...')\n\n    check_wksp_status(response['workspace_id'])\n            \n    return response['workspace_id']", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "def mount_confusion_matrix(test_set,teste_wid):\n    cm_predicted = []\n    cm_predicted_2 = []\n\n    cm_conf_p1 = []\n    cm_conf_p2 = []\n    cm_delta = []\n\n    cm_true = []\n    cm_true_q = []\n\n    for index, row in test_set.iterrows():\n        message = { 'text': row['Example'] }\n        response = conversation.message(workspace_id=teste_wid,input=message,alternate_intents=True)\n\n        if response['intents'] != []:\n            cm_true_q.append(row['Example'])\n            cm_true.append(row['Intent'])\n            cm_predicted.append(response['intents'][0]['intent'])\n            cm_conf_p1.append(response['intents'][0]['confidence'])\n            \n            if len(response['intents']) > 1:\n                cm_predicted_2.append(response['intents'][1]['intent'])\n                cm_conf_p2.append(response['intents'][1]['confidence'])\n                cm_delta.append(float(response['intents'][0]['confidence']) - float(response['intents'][1]['confidence']))\n            else:\n                cm_predicted_2.append('irrelevant')\n                cm_conf_p2.append(0)\n                cm_delta.append(1)\n                \n    resultados = pd.DataFrame({\n        'Question': cm_true_q,\n        'True': cm_true,\n        'Predicted_1': cm_predicted,\n        'Predicted_2': cm_predicted_2,\n        'Conf_1': cm_conf_p1,\n        'Conf_2': cm_conf_p2,\n        'Delta': cm_delta\n    }, columns=['Question','True','Predicted_1','Predicted_2','Conf_1','Conf_2','Delta','Missed'])\n\n    resultados['Missed'] = resultados.apply(lambda x : 'X' if x['True'] != x['Predicted_1'] else '', axis=1)\n    \n    return resultados, cm_true, cm_predicted, cm_predicted_2", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "def plot_confusion_matrix(cm, classes=None, normalize=False, title='', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## ML Execution Results\n\nAs a sampling strategy we run a 80/20 split three times and then calculate resulting scores. If executed only once your results will be unreliable due to the dependency on randomizing sample selection so we do it at least three times to reduce error.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "N_ROUNDS = 1\nrun_wids = []\nrun_total_df_result = pd.DataFrame(columns=['Question','True','Predicted_1','Predicted_2','Conf_1','Conf_2','Delta','Missed'])\nrun_total_true = []\nrun_total_predicted = []\nrun_total_predicted_2 = []\n\nfor run_counter in range(0,N_ROUNDS):\n    \n    # Splits train and test sets with a 80/20 proportion\n    X_train, X_test, y_train, y_test = train_test_split(list_ml_examples, list_ml_examples.Intent, test_size=0.2, stratify=list_ml_examples.Intent)\n    \n    # Executes test workspace creation\n    test_wid = create_test_workspace(X_train)\n    \n    run_temp_result, run_temp_true, run_temp_predicted, run_temp_predicted_2 = mount_confusion_matrix(X_test,test_wid)\n    \n    run_wids.append(test_wid)\n    run_total_df_result = pd.concat([run_total_df_result,run_temp_result])\n    run_total_true = run_total_true + run_temp_true\n    run_total_predicted = run_total_predicted + run_temp_predicted\n    run_total_predicted_2 = run_total_predicted + run_temp_predicted_2\n    \n    run_counter = run_counter + 1\n    \n    # Automatically deletes test workspaces\n    conversation.delete_workspace(workspace_id=test_wid)\n", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Onde ele errou?", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "run_total_df_result[run_total_df_result['Missed'] == 'X'][0:20]", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Confusion", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "run_total_df_result[run_total_df_result['Delta'] < 0.3][0:20]", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## High Confidence Misses", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "run_total_df_result[(run_total_df_result['Conf_1'] > 0.7) & (run_total_df_result['Missed'] == 'X')][0:20]", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "class_names = run_total_df_result['True'].drop_duplicates().tolist()\nclass_names.append('irrelevant')\n\nfigure_size = (50,50)\nmpl.rcParams['figure.figsize'] = figure_size", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Confusion Matrix\n\nTo understand a confusion matrix (cm), you must understand that for each class x:\n* True positives: diagonal position, cm(x, x).\n* False positives: sum of column x (without main diagonal), sum(cm(:, x))-cm(x, x).\n* False negatives: sum of row x (without main diagonal), sum(cm(x, :), 2)-cm(x, x).", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "cnf_matrix = confusion_matrix(run_total_true, run_total_predicted, labels=class_names)\nplot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion Matrix for Intents')", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "print(\"Precision\")\nprint(\"Weighted: \" + str(precision_score(run_total_true, run_total_predicted, average='weighted')))\n\nprint(\"\\n\\nRecall\")\nprint(\"Weighted: \" + str(recall_score(run_total_true, run_total_predicted, average='weighted')))\n\nprint(\"\\n\\nF1\")\nprint(\"Weighted: \" + str(f1_score(run_total_true, run_total_predicted, average='weighted')))\n", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# Metrics for each intent\nprint(classification_report(run_total_true, run_total_predicted))", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": []
        }
    ], 
    "nbformat": 4, 
    "nbformat_minor": 2, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 1.6 (Unsupported)", 
            "name": "python3"
        }, 
        "language_info": {
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "file_extension": ".py", 
            "version": "3.5.2", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }
}